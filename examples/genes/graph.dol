gene Graph<N, E> {
  has nodes: List<N>
  has edges: List<Tuple<UInt64, UInt64, E>>
  has adjacency: SparseMatrix<Float64>
  has node_count: UInt64 = nodes.length
  has edge_count: UInt64 = edges.length

  constraint valid_edges {
    forall edge in this.edges.
      edge.0 < this.nodes.length && edge.1 < this.nodes.length
  }

  constraint no_self_loops {
    forall edge in this.edges.
      edge.0 != edge.1
  }

  law undirected_symmetry {
    forall (i, j, e) in this.edges.
      (j, i, e) in this.edges
  }

  fun neighbors(node_idx: UInt64) -> List<UInt64> {
    return this.edges
      .filter(|edge| edge.0 == node_idx)
      .map(|edge| edge.1)
  }

  fun degree(node_idx: UInt64) -> UInt64 {
    return this.neighbors(node_idx).length
  }

  fun has_edge(src: UInt64, dst: UInt64) -> Bool {
    return this.edges.any(|edge| edge.0 == src && edge.1 == dst)
  }

  fun get_node(idx: UInt64) -> N {
    return this.nodes[idx]
  }

  fun get_edge_data(src: UInt64, dst: UInt64) -> Option<E> {
    return this.edges
      .find(|edge| edge.0 == src && edge.1 == dst)
      .map(|edge| edge.2)
  }
}

exegesis {
  Graph<N, E> models a graph domain where N is the node feature type and
  E is the edge feature type. This is a fundamental structure for Geometric
  Deep Learning architectures operating on relational data.

  This gene represents the Graph domain in the GDL three-pillar ontology,
  enabling permutation-equivariant neural network architectures like
  Graph Neural Networks (GNNs) and Message Passing Neural Networks (MPNNs).

  Properties:
  - nodes: List of node features of type N (e.g., atom embeddings)
  - edges: List of (source, target, edge_data) tuples
  - adjacency: Sparse adjacency matrix for efficient graph operations
  - node_count: Number of nodes (derived)
  - edge_count: Number of edges (derived)

  Edge Validity Constraint:
  The valid_edges constraint ensures all edge indices are within bounds,
  preventing invalid node references. This is critical for memory safety
  and guarantees well-formed graph structure.

  No Self-Loops Constraint:
  The no_self_loops constraint prevents edges from a node to itself,
  which is standard for many graph learning tasks. This can be relaxed
  for domains where self-loops are meaningful.

  Undirected Symmetry Law:
  The undirected_symmetry law declares that for every edge (i, j, e),
  the reverse edge (j, i, e) must also exist. This law encodes the
  mathematical property of undirected graphs and enables symmetric
  message passing in GNN architectures.

  Inherent Symmetry:
  Graph domains have inherent Permutation Group S_n symmetry.
  Any reordering of node indices that preserves edge connectivity
  represents the same graph. Architectures must be permutation-equivariant
  to respect this structure.

  Common Use Cases:
  - Molecular graphs: Graph<AtomFeatures, BondFeatures>
  - Social networks: Graph<UserEmbedding, RelationType>
  - Knowledge graphs: Graph<EntityEmbedding, RelationEmbedding>
  - Citation networks: Graph<PaperEmbedding, CitationType>

  Related Genes:
  - HeteroGraph<N, E>: Heterogeneous graphs with multiple node/edge types
  - DirectedGraph<N, E>: Directed graphs without symmetry law
  - TemporalGraph<N, E, T>: Graphs with temporal edge attributes

  GDL Blueprint Reference:
  Domain type in the three-pillar ontology: Domain (Omega) -> Graph
  Inherent symmetry group: Permutation Group S_n
}
