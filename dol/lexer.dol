module dol.lexer @ 0.4.0

exegesis {
  The DOL lexer tokenizes DOL source code into a stream of tokens.
  This is the foundation of the DOL self-hosting compiler.

  The lexer handles:
  - Keywords and identifiers
  - Numeric literals (integers and floats)
  - String literals with escape sequences
  - All operators (single and multi-character)
  - Line comments (-- style)
  - Whitespace and newlines
}

use dol.token.{ Token, TokenKind, Span, keyword_lookup }

-- =============================================================================
-- Lexer State
-- =============================================================================

exegesis {
  The Lexer gene maintains the state needed during tokenization.
  It tracks position in the source, current line/column for error
  reporting, and accumulates the output token list.
}

/// Lexer state for tokenizing DOL source code
pub gene Lexer {
  /// The source code being tokenized
  has source: String

  /// Current byte position in source
  has pos: UInt64 = 0

  /// Current line number (1-indexed)
  has line: UInt32 = 1

  /// Current column number (1-indexed)
  has column: UInt32 = 1

  /// Accumulated tokens
  has tokens: List<Token> = []
}

-- =============================================================================
-- Character Access Methods
-- =============================================================================

exegesis {
  Get the character at the current position, if any.
}

/// Returns the current character or None if at end
fun current(self: Lexer) -> Option<Char> {
  if self.pos >= self.source.length() {
    return None
  }
  return Some(self.source.char_at(self.pos))
}

exegesis {
  Peek ahead by a given offset without consuming.
}

/// Look ahead by offset characters
fun peek(self: Lexer, offset: UInt64) -> Option<Char> {
  let target = self.pos + offset
  if target >= self.source.length() {
    return None
  }
  return Some(self.source.char_at(target))
}

exegesis {
  Advance the lexer by one character, updating position and column.
  Handles newlines by incrementing line and resetting column.
}

/// Consume and return the current character
fun advance(self: Lexer) -> Option<Char> {
  match self.current() {
    None => None
    Some(c) => {
      self.pos = self.pos + 1
      if c == '\n' {
        self.line = self.line + 1
        self.column = 1
      } else {
        self.column = self.column + 1
      }
      Some(c)
    }
  }
}

-- =============================================================================
-- Token Construction
-- =============================================================================

exegesis {
  Create a token with span information from start position to current.
}

/// Create a token spanning from start to current position
fun make_token(self: Lexer, kind: TokenKind, start: UInt64) -> Token {
  let text = self.source.substring(start, self.pos)
  Token {
    kind: kind,
    text: text,
    span: Span {
      start: start,
      end: self.pos,
      line: self.line,
      column: self.column - (self.pos - start) as UInt32
    }
  }
}

exegesis {
  Create a token and add it to the token list.
}

/// Create and emit a token
fun emit(self: Lexer, kind: TokenKind, start: UInt64) {
  let token = self.make_token(kind, start)
  self.tokens.push(token)
}

-- =============================================================================
-- Whitespace and Comments
-- =============================================================================

exegesis {
  Skip whitespace characters (space, tab, carriage return, newline).
}

/// Skip whitespace characters
fun skip_whitespace(self: Lexer) {
  loop {
    match self.current() {
      None => break
      Some(c) => {
        match c {
          ' ' | '\t' | '\r' | '\n' => {
            self.advance()
          }
          _ => break
        }
      }
    }
  }
}

exegesis {
  Skip a line comment starting with --.
  Comments extend to the end of the line.
}

/// Skip a line comment (-- to end of line)
fun skip_line_comment(self: Lexer) {
  -- Consume the -- prefix (already verified by caller)
  self.advance()  -- first -
  self.advance()  -- second -

  -- Consume until newline or EOF
  loop {
    match self.current() {
      None => break
      Some('\n') => {
        self.advance()
        break
      }
      Some(_) => {
        self.advance()
      }
    }
  }
}

-- =============================================================================
-- Identifier and Keyword Scanning
-- =============================================================================

exegesis {
  Check if a character can start an identifier.
}

fun is_ident_start(c: Char) -> Bool {
  (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || c == '_'
}

exegesis {
  Check if a character can continue an identifier.
}

fun is_ident_continue(c: Char) -> Bool {
  is_ident_start(c) || (c >= '0' && c <= '9')
}

exegesis {
  Scan an identifier or keyword.
  Identifiers start with a letter or underscore and continue with
  letters, digits, or underscores. Keywords are checked via lookup.
}

/// Scan an identifier or keyword
fun scan_identifier(self: Lexer) -> Token {
  let start = self.pos

  -- Consume identifier characters
  loop {
    match self.current() {
      None => break
      Some(c) => {
        if is_ident_continue(c) {
          self.advance()
        } else {
          break
        }
      }
    }
  }

  let text = self.source.substring(start, self.pos)

  -- Check if it's a keyword
  let kind = keyword_lookup(text) ?? TokenKind.Identifier

  self.make_token(kind, start)
}

-- =============================================================================
-- Number Scanning
-- =============================================================================

exegesis {
  Check if a character is a digit.
}

fun is_digit(c: Char) -> Bool {
  c >= '0' && c <= '9'
}

exegesis {
  Scan a numeric literal (integer or float).
  Supports:
  - Integer literals: 42, 0, 1234
  - Float literals: 3.14, 0.5, 123.456
  - Underscores in numbers: 1_000_000
}

/// Scan an integer or float literal
fun scan_number(self: Lexer) -> Token {
  let start = self.pos
  let is_float = false

  -- Consume integer part
  loop {
    match self.current() {
      None => break
      Some(c) => {
        if is_digit(c) || c == '_' {
          self.advance()
        } else {
          break
        }
      }
    }
  }

  -- Check for decimal point
  match self.current() {
    Some('.') => {
      -- Look ahead to ensure it's not a method call (e.g., 42.to_string)
      match self.peek(1) {
        Some(next) => {
          if is_digit(next) {
            is_float = true
            self.advance()  -- consume '.'

            -- Consume fractional part
            loop {
              match self.current() {
                None => break
                Some(c) => {
                  if is_digit(c) || c == '_' {
                    self.advance()
                  } else {
                    break
                  }
                }
              }
            }
          }
        }
        None => {}
      }
    }
    _ => {}
  }

  -- Check for exponent (e or E)
  match self.current() {
    Some('e') | Some('E') => {
      is_float = true
      self.advance()

      -- Optional sign
      match self.current() {
        Some('+') | Some('-') => {
          self.advance()
        }
        _ => {}
      }

      -- Exponent digits
      loop {
        match self.current() {
          None => break
          Some(c) => {
            if is_digit(c) || c == '_' {
              self.advance()
            } else {
              break
            }
          }
        }
      }
    }
    _ => {}
  }

  let kind = if is_float { TokenKind.FloatLit } else { TokenKind.IntLit }
  self.make_token(kind, start)
}

-- =============================================================================
-- String Scanning
-- =============================================================================

exegesis {
  Scan a string literal enclosed in double quotes.
  Supports escape sequences: \n, \t, \r, \\, \", \0
}

/// Scan a string literal
fun scan_string(self: Lexer) -> Token {
  let start = self.pos

  -- Consume opening quote
  self.advance()

  loop {
    match self.current() {
      None => {
        -- Unterminated string
        break
      }
      Some('"') => {
        -- Closing quote
        self.advance()
        break
      }
      Some('\\') => {
        -- Escape sequence
        self.advance()
        match self.current() {
          Some('n') | Some('t') | Some('r') | Some('\\') |
          Some('"') | Some('0') => {
            self.advance()
          }
          Some(_) => {
            -- Invalid escape, continue anyway
            self.advance()
          }
          None => break
        }
      }
      Some('\n') => {
        -- Newline in string (multi-line strings allowed)
        self.advance()
      }
      Some(_) => {
        self.advance()
      }
    }
  }

  self.make_token(TokenKind.StringLit, start)
}

-- =============================================================================
-- Main Token Scanner
-- =============================================================================

exegesis {
  Scan the next token from the source.
  Handles all token types including multi-character operators.
}

/// Scan a single token
fun scan_token(self: Lexer) -> Option<Token> {
  -- Skip whitespace first
  self.skip_whitespace()

  match self.current() {
    None => None
    Some(c) => {
      let start = self.pos

      -- Check for line comments
      if c == '-' {
        match self.peek(1) {
          Some('-') => {
            self.skip_line_comment()
            return self.scan_token()  -- Recurse to get actual token
          }
          _ => {}
        }
      }

      -- Two-character operators (must check before single char)
      match c {
        '-' => {
          match self.peek(1) {
            Some('>') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Arrow, start))
            }
            _ => {}
          }
        }
        '=' => {
          match self.peek(1) {
            Some('>') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.FatArrow, start))
            }
            Some('=') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Eq, start))
            }
            _ => {}
          }
        }
        '|' => {
          match self.peek(1) {
            Some('>') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Pipe, start))
            }
            Some(']') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.RIdiom, start))
            }
            Some('|') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Or, start))
            }
            _ => {}
          }
        }
        '>' => {
          match self.peek(1) {
            Some('>') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Compose, start))
            }
            Some('=') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Ge, start))
            }
            _ => {}
          }
        }
        '<' => {
          match self.peek(1) {
            Some('|') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Map, start))
            }
            Some('=') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Le, start))
            }
            _ => {}
          }
        }
        ':' => {
          match self.peek(1) {
            Some('=') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Bind, start))
            }
            _ => {}
          }
        }
        '!' => {
          match self.peek(1) {
            Some('=') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.Ne, start))
            }
            _ => {}
          }
        }
        '&' => {
          match self.peek(1) {
            Some('&') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.And, start))
            }
            _ => {}
          }
        }
        '*' => {
          match self.peek(1) {
            Some('*') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.StarStar, start))
            }
            _ => {}
          }
        }
        '[' => {
          match self.peek(1) {
            Some('|') => {
              self.advance()
              self.advance()
              return Some(self.make_token(TokenKind.LIdiom, start))
            }
            _ => {}
          }
        }
        _ => {}
      }

      -- Single-character tokens
      match c {
        '{' => {
          self.advance()
          Some(self.make_token(TokenKind.LBrace, start))
        }
        '}' => {
          self.advance()
          Some(self.make_token(TokenKind.RBrace, start))
        }
        '(' => {
          self.advance()
          Some(self.make_token(TokenKind.LParen, start))
        }
        ')' => {
          self.advance()
          Some(self.make_token(TokenKind.RParen, start))
        }
        '[' => {
          self.advance()
          Some(self.make_token(TokenKind.LBracket, start))
        }
        ']' => {
          self.advance()
          Some(self.make_token(TokenKind.RBracket, start))
        }
        ',' => {
          self.advance()
          Some(self.make_token(TokenKind.Comma, start))
        }
        ':' => {
          self.advance()
          Some(self.make_token(TokenKind.Colon, start))
        }
        ';' => {
          self.advance()
          Some(self.make_token(TokenKind.Semicolon, start))
        }
        '.' => {
          self.advance()
          Some(self.make_token(TokenKind.Dot, start))
        }
        '@' => {
          self.advance()
          Some(self.make_token(TokenKind.At, start))
        }
        '+' => {
          self.advance()
          Some(self.make_token(TokenKind.Plus, start))
        }
        '-' => {
          self.advance()
          Some(self.make_token(TokenKind.Minus, start))
        }
        '*' => {
          self.advance()
          Some(self.make_token(TokenKind.Star, start))
        }
        '/' => {
          self.advance()
          Some(self.make_token(TokenKind.Slash, start))
        }
        '%' => {
          self.advance()
          Some(self.make_token(TokenKind.Percent, start))
        }
        '^' => {
          self.advance()
          Some(self.make_token(TokenKind.Caret, start))
        }
        '=' => {
          self.advance()
          Some(self.make_token(TokenKind.Assign, start))
        }
        '<' => {
          self.advance()
          Some(self.make_token(TokenKind.Lt, start))
        }
        '>' => {
          self.advance()
          Some(self.make_token(TokenKind.Gt, start))
        }
        '!' => {
          self.advance()
          Some(self.make_token(TokenKind.Not, start))
        }
        '|' => {
          self.advance()
          Some(self.make_token(TokenKind.Bar, start))
        }
        '&' => {
          self.advance()
          Some(self.make_token(TokenKind.Ampersand, start))
        }
        '?' => {
          self.advance()
          Some(self.make_token(TokenKind.Question, start))
        }
        '#' => {
          self.advance()
          Some(self.make_token(TokenKind.Hash, start))
        }
        '\'' => {
          self.advance()
          Some(self.make_token(TokenKind.Quote, start))
        }
        '"' => {
          Some(self.scan_string())
        }
        _ => {
          -- Identifiers and keywords
          if is_ident_start(c) {
            Some(self.scan_identifier())
          } else if is_digit(c) {
            Some(self.scan_number())
          } else {
            -- Unknown character - emit error token and advance
            self.advance()
            Some(self.make_token(TokenKind.Error, start))
          }
        }
      }
    }
  }
}

-- =============================================================================
-- Public API
-- =============================================================================

exegesis {
  Tokenize DOL source code into a list of tokens.
  This is the main entry point for the lexer.
}

/// Tokenize source code into a list of tokens
pub fun lex(source: String) -> List<Token> {
  let lexer = Lexer { source: source }

  loop {
    match lexer.scan_token() {
      None => break
      Some(token) => {
        lexer.tokens.push(token)
      }
    }
  }

  -- Add EOF token
  let eof = Token {
    kind: TokenKind.Eof,
    text: "",
    span: Span {
      start: lexer.pos,
      end: lexer.pos,
      line: lexer.line,
      column: lexer.column
    }
  }
  lexer.tokens.push(eof)

  lexer.tokens
}

-- =============================================================================
-- Tests
-- =============================================================================

#[test]
fun test_empty_source() {
  let tokens = lex("")
  assert tokens.length() == 1
  assert tokens[0].kind == TokenKind.Eof
}

#[test]
fun test_single_identifier() {
  let tokens = lex("foo")
  assert tokens.length() == 2
  assert tokens[0].kind == TokenKind.Identifier
  assert tokens[0].text == "foo"
  assert tokens[1].kind == TokenKind.Eof
}

#[test]
fun test_keywords() {
  let tokens = lex("gene trait system fun")
  assert tokens[0].kind == TokenKind.Gene
  assert tokens[1].kind == TokenKind.Trait
  assert tokens[2].kind == TokenKind.System
  assert tokens[3].kind == TokenKind.Fun
}

#[test]
fun test_numbers() {
  let tokens = lex("42 3.14 1_000_000")
  assert tokens[0].kind == TokenKind.IntLit
  assert tokens[0].text == "42"
  assert tokens[1].kind == TokenKind.FloatLit
  assert tokens[1].text == "3.14"
  assert tokens[2].kind == TokenKind.IntLit
  assert tokens[2].text == "1_000_000"
}

#[test]
fun test_string_literal() {
  let tokens = lex("\"hello world\"")
  assert tokens[0].kind == TokenKind.StringLit
  assert tokens[0].text == "\"hello world\""
}

#[test]
fun test_operators() {
  let tokens = lex("-> => |> >> <| := == != <= >= && || **")
  assert tokens[0].kind == TokenKind.Arrow
  assert tokens[1].kind == TokenKind.FatArrow
  assert tokens[2].kind == TokenKind.Pipe
  assert tokens[3].kind == TokenKind.Compose
  assert tokens[4].kind == TokenKind.Map
  assert tokens[5].kind == TokenKind.Bind
  assert tokens[6].kind == TokenKind.Eq
  assert tokens[7].kind == TokenKind.Ne
  assert tokens[8].kind == TokenKind.Le
  assert tokens[9].kind == TokenKind.Ge
  assert tokens[10].kind == TokenKind.And
  assert tokens[11].kind == TokenKind.Or
  assert tokens[12].kind == TokenKind.StarStar
}

#[test]
fun test_idiom_brackets() {
  let tokens = lex("[| |]")
  assert tokens[0].kind == TokenKind.LIdiom
  assert tokens[1].kind == TokenKind.RIdiom
}

#[test]
fun test_line_comment() {
  let tokens = lex("foo -- this is a comment\nbar")
  assert tokens.length() == 3
  assert tokens[0].kind == TokenKind.Identifier
  assert tokens[0].text == "foo"
  assert tokens[1].kind == TokenKind.Identifier
  assert tokens[1].text == "bar"
}

#[test]
fun test_complex_expression() {
  let tokens = lex("x := y |> f >> g")
  assert tokens[0].kind == TokenKind.Identifier
  assert tokens[1].kind == TokenKind.Bind
  assert tokens[2].kind == TokenKind.Identifier
  assert tokens[3].kind == TokenKind.Pipe
  assert tokens[4].kind == TokenKind.Identifier
  assert tokens[5].kind == TokenKind.Compose
  assert tokens[6].kind == TokenKind.Identifier
}
