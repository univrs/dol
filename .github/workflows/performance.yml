name: Performance Tests

on:
  push:
    branches: [ main, feature/* ]
  pull_request:
    branches: [ main ]

env:
  CARGO_TERM_COLOR: always

jobs:
  performance-regression:
    name: Performance Regression Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true

      - name: Cache cargo registry
        uses: actions/cache@v3
        with:
          path: ~/.cargo/registry
          key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}

      - name: Cache cargo index
        uses: actions/cache@v3
        with:
          path: ~/.cargo/git
          key: ${{ runner.os }}-cargo-index-${{ hashFiles('**/Cargo.lock') }}

      - name: Cache cargo build
        uses: actions/cache@v3
        with:
          path: target
          key: ${{ runner.os }}-cargo-build-target-${{ hashFiles('**/Cargo.lock') }}

      - name: Run performance regression tests
        run: |
          cd benchmarks
          cargo test --release --test regression_tests -- --nocapture

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-test-results
          path: |
            target/criterion/
            target/*.txt

  benchmark-comparison:
    name: Benchmark Comparison
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v3

      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true

      - name: Install cargo-criterion
        run: cargo install cargo-criterion

      - name: Run benchmarks (PR)
        run: |
          cd benchmarks
          cargo criterion --message-format=json > ../pr-benchmarks.json || true

      - name: Checkout main branch
        uses: actions/checkout@v3
        with:
          ref: main

      - name: Run benchmarks (main)
        run: |
          cd benchmarks
          cargo criterion --message-format=json > ../main-benchmarks.json || true

      - name: Compare benchmarks
        run: |
          echo "## Benchmark Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Comparing PR against main branch..." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "See artifact 'benchmark-comparison' for detailed results." >> $GITHUB_STEP_SUMMARY

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-comparison
          path: |
            pr-benchmarks.json
            main-benchmarks.json
            target/criterion/

  wasm-size-check:
    name: WASM Size Budget Check
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          target: wasm32-unknown-unknown
          profile: minimal
          override: true

      - name: Install wasm-opt
        run: |
          wget https://github.com/WebAssembly/binaryen/releases/download/version_116/binaryen-version_116-x86_64-linux.tar.gz
          tar -xzf binaryen-version_116-x86_64-linux.tar.gz
          sudo mv binaryen-version_116/bin/wasm-opt /usr/local/bin/

      - name: Build WASM modules
        run: |
          cargo build --target wasm32-unknown-unknown --release --features wasm-compile

      - name: Check WASM sizes
        run: |
          echo "## WASM Module Sizes" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Module | Size (KB) | Gzipped (KB) | Budget | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-----------|--------------|--------|--------|" >> $GITHUB_STEP_SUMMARY

          for wasm in target/wasm32-unknown-unknown/release/*.wasm; do
            if [ -f "$wasm" ]; then
              name=$(basename "$wasm")
              size_kb=$(($(stat -c%s "$wasm") / 1024))
              gzip -c "$wasm" > "$wasm.gz"
              gzip_kb=$(($(stat -c%s "$wasm.gz") / 1024))

              if [ $gzip_kb -lt 100 ]; then
                status="✅ PASS"
              else
                status="❌ FAIL"
              fi

              echo "| $name | $size_kb | $gzip_kb | <100 | $status |" >> $GITHUB_STEP_SUMMARY
            fi
          done

      - name: Optimize WASM with wasm-opt
        run: |
          for wasm in target/wasm32-unknown-unknown/release/*.wasm; do
            if [ -f "$wasm" ]; then
              ./scripts/optimize-wasm.sh "$wasm" || true
            fi
          done

      - name: Upload WASM modules
        uses: actions/upload-artifact@v3
        with:
          name: wasm-modules
          path: |
            target/wasm32-unknown-unknown/release/*.wasm
            target/wasm32-unknown-unknown/release/*.wasm.gz

  memory-profile:
    name: Memory Profiling
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true

      - name: Install Valgrind
        run: sudo apt-get install -y valgrind

      - name: Run memory benchmarks with Valgrind
        run: |
          cd benchmarks
          valgrind --tool=massif --massif-out-file=massif.out \
            cargo test --release test_memory_usage_budget -- --nocapture || true

      - name: Generate memory report
        run: |
          if [ -f benchmarks/massif.out ]; then
            ms_print benchmarks/massif.out > memory-report.txt

            echo "## Memory Profiling Report" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            head -n 50 memory-report.txt >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload memory profile
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: memory-profile
          path: |
            benchmarks/massif.out
            memory-report.txt

  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: [performance-regression, wasm-size-check, memory-profile]
    if: always()

    steps:
      - name: Generate summary
        run: |
          echo "# Performance Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Regression Tests: ${{ needs.performance-regression.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- WASM Size Check: ${{ needs.wasm-size-check.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Memory Profile: ${{ needs.memory-profile.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.performance-regression.result }}" == "success" ] && \
             [ "${{ needs.wasm-size-check.result }}" == "success" ] && \
             [ "${{ needs.memory-profile.result }}" == "success" ]; then
            echo "✅ All performance tests passed!" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Some performance tests failed. See individual job results for details." >> $GITHUB_STEP_SUMMARY
          fi
